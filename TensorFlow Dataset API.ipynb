{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34976a85",
   "metadata": {},
   "source": [
    "# TensorFlow Dataset API\n",
    "\n",
    "**Learning Objectives**\n",
    "1. Learn how to use tf.data to read data from memory\n",
    "1. Learn how to use tf.data in a training loop\n",
    "1. Learn how to use tf.data to read data from disk\n",
    "1. Learn how to write production input pipelines with feature engineering (batching, shuffling, etc.)\n",
    "\n",
    "\n",
    "In this notebook, we will start by refactoring the linear regression we implemented in the previous lab so that it takes data from a`tf.data.Dataset`, and we will learn how to implement **stochastic gradient descent** with it. In this case, the original dataset will be synthetic and read by the `tf.data` API directly  from memory.\n",
    "\n",
    "In a second part, we will learn how to load a dataset with the `tf.data` API when the dataset resides on disk.\n",
    "\n",
    "Each learning objective will correspond to a __#TODO__ in the [student lab notebook](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive2/introduction_to_tensorflow/labs/2_dataset_api.ipynb) -- try to complete that notebook first before reviewing this solution notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05127a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import json # The json module is mainly used to convert the python dictionary above into a JSON string that can written in a file\n",
    "import os # Interact withh the operating system\n",
    "import math # python module provides mathematical functions\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version: \", tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1161318",
   "metadata": {},
   "source": [
    "#### Objective 1: Loading data from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403fb891",
   "metadata": {},
   "source": [
    "#### Creating the dataset\n",
    "\n",
    "Let us consider the synthetic dataset of the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad392b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 20\n",
    "# constant() method will create a constant tensor from tensor-like objects\n",
    "X = tf.constant(range(n_points), dtype = tf.float16)\n",
    "Y = 3 * X - 5\n",
    "pprint(X)\n",
    "pprint(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32260f68",
   "metadata": {},
   "source": [
    "We begin with implementing a function that takes as input\n",
    "\n",
    "\n",
    "- our $X$ and $Y$ vectors of synthetic data generated by the linear function $y= 2x + 10$\n",
    "- the number of passes over the dataset we want to train on (`epochs`)\n",
    "- the size of the batches the dataset (`batch_size`)\n",
    "\n",
    "and returns a `tf.data.Dataset`: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9105f655",
   "metadata": {},
   "source": [
    "**Remark:** Note that the last batch may not contain the exact number of elements you specified because the dataset was exhausted.\n",
    "\n",
    "If you want batches with the exact same number of elements per batch, we will have to discard the last batch by\n",
    "setting:\n",
    "\n",
    "```python\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "```\n",
    "\n",
    "We will do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b3ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset procedure\n",
    "\n",
    "def new_dataset(X, Y, epochs, batch_size):\n",
    "    #Using tf.data.dataset.from_tensor_slices to get slices of list or array\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc4bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the function by iterating thrice over our dataset in batches of 6 points\n",
    "\n",
    "EPOCH = 6\n",
    "BATCH_SIZE = 6\n",
    "\n",
    "dataset = new_dataset(X, Y, epochs=EPOCH, batch_size=BATCH_SIZE)\n",
    "pprint(dataset)\n",
    "\n",
    "for i, (x, y) in enumerate(dataset):\n",
    "    print(\"x: \", x.numpy(), \"y: \", y.numpy())\n",
    "    assert len(x) == BATCH_SIZE\n",
    "    assert len(y) == BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336d15f1",
   "metadata": {},
   "source": [
    "#### Lost Function and Gradients\n",
    "\n",
    "The lost function anf the function that computes the gradients are the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f47dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us define loss_mse() procedure which will return computed mean of elements across dimensions of a tensor.\n",
    "\n",
    "def loss_mse(X, Y, w0, w1):\n",
    "    Y_hat = w0 * X + w1\n",
    "    error = (Y_hat - Y)**2\n",
    "    return tf.reduce_mean(error)\n",
    "\n",
    "# Define compute_gradient() procedure which will return value of recorded operations for automatic differentiation\n",
    "def compute_gradient(X, Y, w0, w1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_mse(X, Y, w0, w1)\n",
    "    return tape.gradient(loss, [w0, w1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38780c7d",
   "metadata": {},
   "source": [
    "#### Objective 2: Use tf.data in Training Loop\n",
    "\n",
    "The main difference now is that now, in the training loop, we will iterate directly on the tf.data.Dataset generated by our create_dataset function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beebb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we will configure the dataset so that it iterates 250 times over our synthetic dataset of batches 6\n",
    "\n",
    "EPOCHS = 300\n",
    "BATCH_SIZE = 6\n",
    "LEARNING_RATE = .02\n",
    "\n",
    "MSG = \"STEP {step} - loss: {loss}, w0: {w0}, w1: {w1}\\n\"\n",
    "\n",
    "w0 = tf.Variable(0.0, dtype=tf.float16)\n",
    "w1 = tf.Variable(0.0, dtype=tf.float16)\n",
    "\n",
    "dataset = new_dataset(X, Y, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "for step, (X_batch, Y_batch) in enumerate(dataset):\n",
    "    \n",
    "    dw0, dw1 = compute_gradient(X_batch, Y_batch, w0, w1)\n",
    "    w0.assign_sub(dw0 * LEARNING_RATE)\n",
    "    w1.assign_sub(dw1 * LEARNING_RATE)\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        loss = loss_mse(X_batch, Y_batch, w0, w1)\n",
    "        print(MSG.format(step=step, loss=loss, w0=w0.numpy(), w1=w1.numpy()))\n",
    "        \n",
    "assert loss < 0.0001\n",
    "assert abs(w0 -2) < 0.001\n",
    "assert abs(w1 - 10) < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5875f639",
   "metadata": {},
   "source": [
    "#### Objective 3: Loading data from Disk\n",
    "\n",
    "##### Locating the CSV files\n",
    "\n",
    "\n",
    "The taxifare dataset files have been saves into ../data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86233019",
   "metadata": {},
   "source": [
    "#### Use the tf.data to load the csv files\n",
    "\n",
    "The tf.data API can be easily read csv files using the helper function tf.data.experimental.make_csv_dataset\n",
    "\n",
    "If you have TFRecords(which is recommended), you may use tf.data.experimental.make_batched_features_dataset\n",
    "\n",
    "The first step is to define:\n",
    "    - the feature names into a list CSV_COLUMNS\n",
    "    - their default values into a list DEFAULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9894cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining column names into a list CSV_COLUMNS\n",
    "\n",
    "CSV_COLUMNS = [\n",
    "    'fare_amount',\n",
    "    'pickup_datetime',\n",
    "    'pickup_longitude',\n",
    "    'pickup_latitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'passenger_count',\n",
    "    'key'\n",
    "]\n",
    "\n",
    "# define label column\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "\n",
    "# Define default values into the list DEFAULTS\n",
    "DEFAULTS = [\n",
    "    [0.0], ['na'], [0.0], [0.0], [0.0], [0.0], [0.0], ['na']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aecce5",
   "metadata": {},
   "source": [
    "Wrap the call to make_csv_dataset into its own function that will take only the file pattern(glob) where the dataset files are to be located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0d55a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(pattern):\n",
    "    #tf.data.experimental.make_csv_dataset() method reads CSV files into a dataset\n",
    "    return tf.data.experimental.make_csv_dataset(pattern, 1, column_names=CSV_COLUMNS, column_defaults=DEFAULTS)\n",
    "\n",
    "\n",
    "tempds = create_dataset('taxi-test.csv')\n",
    "print(tempds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b3dbf2",
   "metadata": {},
   "source": [
    "Note that the output is a prefetch dataset where each element is an OrderedDict whose keys are the feature names and whose values are tensors of shape (1,) --vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aa47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the first two elements in the dataset using dataset.take(2)\n",
    "# Convert to python dictionary to numpy for readibality\n",
    "\n",
    "for data in tempds.take(2):\n",
    "    pprint({k: v.numpy() for k, v in data.items()})\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49a25b",
   "metadata": {},
   "source": [
    "#### Objective 4 - Creating Input Pipelines - Transforming the features\n",
    "\n",
    "Objective is to create a dictionary of features plus label. So, we are going to\n",
    "1. Remove the unwanted key\n",
    "2. Keep the label separate from the features\n",
    "    \n",
    "Let's first implement a function that takes as input a row (represented as an OrderedDict in our tf.data.Dataset as above) and then returns a tuple with two elements:\n",
    "\n",
    "- The fist element being the same OrderedDict with the label dropped\n",
    "- The second element being the label itself - fare_amount\n",
    "\n",
    "Note that we will need to also remove the key and pickup_datetime column, which we wont use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNWANTED_COLS = ['pickup_datetime', 'key']\n",
    "\n",
    "# let's define the features_and_labels() method\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    features = row_data\n",
    "    \n",
    "    for unwanted_col in UNWANTED_COLS:\n",
    "        features.pop(unwanted_col)\n",
    "    \n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af322194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate for two examples in tempds to make sure it is working as expected\n",
    "for row_data in tempds.take(2):\n",
    "    features, label = features_and_labels(row_data)\n",
    "    pprint(features)\n",
    "    print(label, \"\\n\")\n",
    "    assert UNWANTED_COLS[0] not in features.keys()\n",
    "    assert UNWANTED_COLS[1] not in features.keys()\n",
    "    assert label.shape == [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca9d02e",
   "metadata": {},
   "source": [
    "#### Batching\n",
    "\n",
    "Let's now refactor our `create_dataset` function so that it takes an additional argument `batch_size` and batch the data correspondingly. We will also use the `features_and_labels` function we implemented for our dataset to produce tuples of features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d53ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define create_dataset method:\n",
    "\n",
    "def create_dataset(pattern, batch_size):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "    pattern, batch_size, CSV_COLUMNS, DEFAULTS)\n",
    "    return dataset.map(features_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test that our batches are of the right size\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "tempds = create_dataset('taxi-test.csv', batch_size=BATCH_SIZE)\n",
    "\n",
    "for X_batch, Y_batch in tempds.take(2):\n",
    "    pprint({k: v.numpy() for k, v in X_batch.items()})\n",
    "    print(Y_batch.numpy(), \"\\n\")\n",
    "    assert len(Y_batch) == BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43482f22",
   "metadata": {},
   "source": [
    "#### Shuffling\n",
    "\n",
    "When training a deep learning model in batches over multiple workers, it is helpful if we shuffle the data. That way, different workers will be working on different parts of the input file at the same time, and so averaging gradients across workers will help. Also, during training, we will need to read the data indefinitely.\n",
    "\n",
    "\n",
    "Let's refactor our `create_dataset` function so that it shuffles the data, when the dataset is used for training.\n",
    "\n",
    "We will introduce an additional argument `mode` to our function to allow the function body to distinguish the case\n",
    "when it needs to shuffle the data (`mode == \"train\"`) from when it shouldn't (`mode == \"eval\"`).\n",
    "\n",
    "Also, before returning we will want to prefetch 1 data point ahead of time (`dataset.prefetch(1)`) to speed-up training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede0056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(pattern, batch_size=1, mode='eval'):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(pattern, batch_size, CSV_COLUMNS, DEFAULTS)\n",
    "    \n",
    "    dataset = dataset.map(features_and_labels)\n",
    "    \n",
    "    if mode == 'train':\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "    \n",
    "    #take advantage of the multi-threading; 1=AUTOTUNE\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0370c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the function is working as expected\n",
    "\n",
    "tempds = create_dataset('taxi-train.csv', 2, \"train\")\n",
    "print(list(tempds.take(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b160b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempds = create_dataset('taxi-valid.csv', 3, \"eval\")\n",
    "print(list(tempds.take(3)))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
